{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Описание **\n",
    "Построить графики распределения в спам и не спам множествах следующих признаков:\n",
    "\n",
    "•\tКоличество слов на странице\n",
    "•\tКоличество слов в заголовке страниц (слова в теге <html><head><title > Some text </title>)\n",
    "•\tСредняя длинна слова\n",
    "•\tКоличество слов в анкорах ссылок (<html><body><a> Some text </a>) # Общее кол-во или среднее кол-во?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import base64\n",
    "import random\n",
    "import chardet\n",
    "import codecs\n",
    "import re\n",
    "from re import sub\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_params(estimator, params, x_train, y_train, scoring=\"roc_auc\", cv=5):\n",
    "    gsc = GridSearchCV(estimator, params, cv=cv, scoring=scoring)\n",
    "    gsc.fit(x_train, y_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(gsc.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in gsc.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "    return gsc.best_estimator_, gsc.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** HTML Parser **\n",
    "Тут мы можем распарсить страницу, выделить текст, заголовок, текст в ссылках документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SpamHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.__text = []\n",
    "        \n",
    "        self.data = defaultdict(list)\n",
    "        self.curr_tag = None\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'img':\n",
    "            d = dict(attrs)\n",
    "            if 'alt' in d.keys():\n",
    "                if d.get('alt'):\n",
    "                    self.data[self.curr_tag].append(d.get('alt'))\n",
    "        \n",
    "        if tag == 'meta':\n",
    "            if attrs:\n",
    "                for k, v in attrs:\n",
    "                    if v:\n",
    "                        self.data[self.curr_tag].append(v)\n",
    "        if tag == None:\n",
    "            print \"ahtung!\"\n",
    "        self.curr_tag = tag\n",
    "            \n",
    "    def handle_data(self, data):\n",
    "        if data:\n",
    "            self.data[self.curr_tag].append(data)\n",
    "        \n",
    "        \n",
    "#         if self.curr_tag == 'meta':\n",
    "#             return\n",
    "        \n",
    "#         if self.curr_tag == 'iframe':\n",
    "#             print data\n",
    "            \n",
    "#         if self.curr_tag:\n",
    "#             self.data[self.curr_tag].append(data)\n",
    "\n",
    "#         text = data.strip()\n",
    "#         if len(text) > 0:\n",
    "#             text = sub('[ \\t\\r\\n]+', ' ', text)\n",
    "#             self.__text.append(text + ' ')\n",
    "\n",
    "            \n",
    "    def handle_endtag(self, tag):\n",
    "        self.curr_tag = None\n",
    "\n",
    "    def text(self):\n",
    "        return ''.join(self.__text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Сбор статистики **\n",
    "Класс отвечает за сбор статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Stat:\n",
    "    def __init__(self):\n",
    "            self.points = []\n",
    "            self.spam = []\n",
    "            self.notspam = []\n",
    "                            \n",
    "def extract_russian(arr):\n",
    "    if len(arr) == 0: return []\n",
    "    text = \" \".join(arr)\n",
    "    russian_words = re.findall(u\"[\\u0400-\\u0500]+\", text.lower())\n",
    "    return [w for w in russian_words if w not in stopwords.words('russian')] \n",
    "                \n",
    "class StatsCollector:\n",
    "    def __init__(self):\n",
    "        self.total_words = Stat();\n",
    "        self.header_words = Stat();\n",
    "        self.average_word_length = Stat();\n",
    "        self.links_words = Stat();\n",
    "        \n",
    "    def collect(self, mark, pageInb64, url):\n",
    "        html = base64.b64decode(pageInb64).decode('utf-8')\n",
    "        \n",
    "        parser = SpamHTMLParser()\n",
    "        parser.feed(html)\n",
    "        \n",
    "    \n",
    "        with codecs.open(\"tmp.html\", \"w\", \"utf-8\") as fout:\n",
    "            fout.write(html)\n",
    "\n",
    "        features = defaultdict(dict)\n",
    "        \n",
    "        for key, val in parser.data.items():\n",
    "            try:\n",
    "                words = extract_russian(val)\n",
    "                if len(words) != 0:\n",
    "                    features[key][\"words\"] = words\n",
    "                    features[key][\"counts\"] = len(val)\n",
    "                    features[key][\"mean\"] = sum([len(body.split()) for body in val])/float(len(val))\n",
    "            except TypeError:\n",
    "                print key, val\n",
    "                raise TypeError\n",
    "\n",
    "#                 print key, len(words), len(val), \" | \",\n",
    "#             if key == \"br\":\n",
    "#                 for w in words:\n",
    "#                     print w\n",
    "        \n",
    "            \n",
    "#         print \"\\n\\n\"\n",
    "#         total_words = int(len(parser.text().split(\" \")))\n",
    "#         if \"title\" in parser.data.keys():\n",
    "#             header_words = int(np.sum(parser.data[\"title\"]))\n",
    "#         else:\n",
    "#             header_words = 0\n",
    "#         average_word_length = int(np.mean([len(w) for w in parser.text().split(\" \")]))\n",
    "#         if \"a\" in parser.data.keys():\n",
    "#             links_words = int(np.sum(parser.data[\"a\"]))\n",
    "#         else:\n",
    "#             links_words = 0\n",
    "    \n",
    "#         if mark == 0: # notspam\n",
    "#             self.total_words.notspam.append(total_words)\n",
    "#             self.header_words.notspam.append(header_words)\n",
    "#             self.average_word_length.notspam.append(average_word_length)\n",
    "#             self.links_words.notspam.append(links_words)\n",
    "            \n",
    "#         if mark == 1: # notspam\n",
    "#             self.total_words.spam.append(total_words)\n",
    "#             self.header_words.spam.append(header_words)\n",
    "#             self.average_word_length.spam.append(average_word_length)\n",
    "#             self.links_words.spam.append(links_words)\n",
    "        \n",
    "#         self.total_words.points.append(mark)\n",
    "#         self.header_words.points.append(mark)\n",
    "#         self.average_word_length.points.append(mark)\n",
    "#         self.links_words.points.append(mark)\n",
    "        \n",
    "        parser.close();\n",
    "        \n",
    "        return features\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Обрабатываем входной файл **\n",
    "Входной файл  ./data/train-set-ru-b64-utf-8.txt \n",
    "Формат - поля разделенные табуляциями\n",
    "0 - идентификатор документа\n",
    "1 - метка класса 0 - не спам, 1 - спам\n",
    "2 - урл документа\n",
    "3 - документ в кодировке base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_FILE  = './data/train-set-ru-b64-utf-8.txt'\n",
    "\n",
    "stats_collector = StatsCollector()\n",
    "i_from=0\n",
    "\n",
    "from urlparse import urlparse\n",
    "from itertools import chain\n",
    "\n",
    "with open(\"urls.txt\", 'w') as furls:\n",
    "    with open (DATA_FILE) as df:\n",
    "        for i, line in enumerate(df):\n",
    "            if i < i_from:\n",
    "                continue\n",
    "\n",
    "            line = line.strip()\n",
    "            parts = line.split()\n",
    "#             print int(parts[1]), parts[2]\n",
    "            url_o = urlparse(parts[2])\n",
    "            url_tokens = re.findall(r\"[a-z]+\", url_o.netloc) + re.findall(r\"[a-z]+\", url_o.path)\n",
    "            url_tokens = [token for token in url_tokens if token not in [\"ru\", \"html\", \"com\"]]\n",
    "            url_tokens = filter(lambda x: len(x) > 3, url_tokens)\n",
    "    #         tri_gramms = [[''.join(b) for b in zip(w, w[1:], w[2:], w[3:])] for w in url_tokens]\n",
    "            data[parts[0]][\"url_tokens\"] = url_tokens\n",
    "\n",
    "    #         print url_tokens\n",
    "\n",
    "    #         features = stats_collector.collect(int(parts[1]), parts[3], parts[2])\n",
    "\n",
    "    #         data[parts[0]][\"target\"] = int(parts[1])\n",
    "    #         data[parts[0]][\"url\"] = parts[2]\n",
    "    #         data[parts[0]][\"features\"] = features\n",
    "\n",
    "    #         sys.stdout.write('\\r' + \"%s\" % (i))\n",
    "    #         sys.stdout.flush()\n",
    "\n",
    "    #         \n",
    "            furls.write(\"%d %s\\n\" % (int(parts[1]), parts[2]))\n",
    "#             if i >= 200:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected string, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-ed13637afa37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected string, NoneType found"
     ]
    }
   ],
   "source": [
    "\" \".join([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# with open('data.json', 'w') as outfile:\n",
    "#     json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = [v[\"target\"] for k, v in data.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "\n",
    "for item in data.items():\n",
    "    tags += item[1][\"features\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_tags = [None, \"p\", \"div\", \"a\", \"title\", \"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\n",
    "                \"span\", \"strong\", \"em\", \"img\", \"b\", \"meta\",\n",
    "                \"i\", \"small\", \"head\", \"blockquote\",\n",
    "               \"cite\", \"big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys, _ = zip(*filter(lambda v: v[1] > 0, Counter(tags).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'a',\n",
       " u'title',\n",
       " u'null',\n",
       " u'div',\n",
       " u'b',\n",
       " u'p',\n",
       " u'h1',\n",
       " u'span',\n",
       " u'strong',\n",
       " u'script',\n",
       " u'td',\n",
       " u'br',\n",
       " u'h2',\n",
       " u'meta',\n",
       " u'h3',\n",
       " u'i',\n",
       " u'font',\n",
       " u'em',\n",
       " u'label',\n",
       " u'li',\n",
       " u'img',\n",
       " u'small',\n",
       " u'h4',\n",
       " u'option',\n",
       " u'th',\n",
       " u'u',\n",
       " u'h5',\n",
       " u'center',\n",
       " u'input',\n",
       " u'nobr',\n",
       " u'ins',\n",
       " u'dfn',\n",
       " u'dt',\n",
       " u'link',\n",
       " u'blockquote',\n",
       " u'form',\n",
       " u'h6',\n",
       " u'cite',\n",
       " u'button',\n",
       " u'noindex',\n",
       " u'big',\n",
       " u'head',\n",
       " u'style',\n",
       " u'base',\n",
       " u'dd',\n",
       " u'textarea',\n",
       " u'noscript',\n",
       " u'legend',\n",
       " u'pre',\n",
       " u'abbr',\n",
       " u'caption',\n",
       " u'hr',\n",
       " u'index',\n",
       " u'ul',\n",
       " u'marquee',\n",
       " u'left',\n",
       " u'address',\n",
       " u'article',\n",
       " u'sup',\n",
       " u's',\n",
       " u'blink',\n",
       " u'quote',\n",
       " u'st1:metricconverter',\n",
       " u'sub',\n",
       " u'idxskip',\n",
       " u'code',\n",
       " u'iframe',\n",
       " u'wbr',\n",
       " u'acronym',\n",
       " u'tt',\n",
       " u'body',\n",
       " u'param',\n",
       " u'section',\n",
       " u'yabarhighlightertag',\n",
       " u'fieldset',\n",
       " u'del',\n",
       " u'tdclass=\"dadeno\"',\n",
       " u'cnter',\n",
       " u'tr',\n",
       " u'tdid=\"ubabut\"',\n",
       " u'norb',\n",
       " u'dl',\n",
       " u'string',\n",
       " u'tdclass=\"adapusi\"',\n",
       " u'tdid=\"adotuno\"',\n",
       " u'right',\n",
       " u'tdclass=\"lofib\"',\n",
       " u'odg',\n",
       " u'basefont',\n",
       " u'bb',\n",
       " u'stong',\n",
       " u'stations',\n",
       " u'tdclass=\"ravono\"',\n",
       " u'nofollow',\n",
       " u'tdid=\"siko\"',\n",
       " u\"aid='theme_butt'\",\n",
       " u'tdclass=\"goned\"',\n",
       " u'tdclass=\"menuvus\"',\n",
       " u\"color=''\",\n",
       " u'table',\n",
       " u'palign=justify',\n",
       " u'strike',\n",
       " u'st:',\n",
       " u'lable',\n",
       " u'phone',\n",
       " u'hidden',\n",
       " u'tdclass=\"efukiba\"',\n",
       " u'tdclass=\"rosagid\"',\n",
       " u'tdid=\"ubidem\"',\n",
       " u'layer',\n",
       " u'tdclass=\"bedo\"',\n",
       " u'tdclass=\"leges\"',\n",
       " u'o:p',\n",
       " u'if',\n",
       " u'work_hours_j',\n",
       " u'tbody',\n",
       " u'work_hours_p',\n",
       " u'nav',\n",
       " u'samp',\n",
       " u'object',\n",
       " u'logistepas',\n",
       " u'class')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Только на числовых"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# кол-во слов\n",
    "# кол-во каждого из тегов > 65\n",
    "# общее кол-во тегов < 65\n",
    "# среднее кро-во слов в теге"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_data = []\n",
    "\n",
    "for id, item in data.items():\n",
    "    obj = []\n",
    "    obj.append(item[\"target\"])\n",
    "    \n",
    "    for key in set(feature_tags):# - set([None, \"p\", \"div\"]):\n",
    "        if key in item[\"features\"].keys():\n",
    "            obj.append(item[\"features\"][key][\"counts\"])\n",
    "            obj.append(item[\"features\"][key][\"mean\"])\n",
    "            obj.append(len(item[\"features\"][key][\"words\"]))\n",
    "            obj.append(len(Counter(item[\"features\"][key][\"words\"]).keys()))\n",
    "        else:\n",
    "            obj.append(0)\n",
    "            obj.append(0)\n",
    "            obj.append(0)\n",
    "            obj.append(0)\n",
    "            \n",
    "    numeric_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([93])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([len(a) for a in numeric_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_data = np.array(numeric_data, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data1[:, 1:], data1[:, 0], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(penalty=\"l1\", C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8756679729248309"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "ccc = CalibratedClassifierCV(lg)\n",
    "ccc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87030138841855742"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, ccc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "              random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92149669845928106"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gb.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93868450390189517"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfc.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'p',\n",
       " 'div',\n",
       " 'a',\n",
       " 'title',\n",
       " 'h1',\n",
       " 'h2',\n",
       " 'h3',\n",
       " 'h4',\n",
       " 'h5',\n",
       " 'h6',\n",
       " 'span',\n",
       " 'strong',\n",
       " 'em',\n",
       " 'img',\n",
       " 'b',\n",
       " 'meta',\n",
       " 'i',\n",
       " 'small',\n",
       " 'head',\n",
       " 'blockquote',\n",
       " 'cite',\n",
       " 'big']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for id, item in data.items():\n",
    "    bag = []\n",
    "    \n",
    "    for key in keys:\n",
    "        if key in item[\"features\"].keys():\n",
    "            words = item[\"features\"][key][\"words\"]\n",
    "            bag += words\n",
    "            bag += [''.join(b) for b in zip(words, words[1:])]\n",
    "            bag += [''.join(b) for b in zip(words, words[1:], words[2:])]\n",
    "        \n",
    "#     bag += item[\"url_tokens\"]\n",
    "\n",
    "    text_data.append(\" \".join(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(use_idf=True, ngram_range=(1, 1), sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = tfidf.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7044, 10387671)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data2, data1[:, 0], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(penalty=\"l1\", C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96098104793756967"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lg.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "ccc = CalibratedClassifierCV(lg)\n",
    "ccc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/model.pkl',\n",
       " './models/model.pkl_01.npy',\n",
       " './models/model.pkl_02.npy',\n",
       " './models/model.pkl_03.npy',\n",
       " './models/model.pkl_04.npy',\n",
       " './models/model.pkl_05.npy',\n",
       " './models/model.pkl_06.npy',\n",
       " './models/model.pkl_07.npy',\n",
       " './models/model.pkl_08.npy',\n",
       " './models/model.pkl_09.npy',\n",
       " './models/model.pkl_10.npy',\n",
       " './models/model.pkl_11.npy',\n",
       " './models/model.pkl_12.npy',\n",
       " './models/model.pkl_13.npy',\n",
       " './models/model.pkl_14.npy',\n",
       " './models/model.pkl_15.npy',\n",
       " './models/model.pkl_16.npy']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ccc, './models/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96399099774943731"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ccc.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95868998883513223"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lsvc.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=0.4)\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92717584369449391"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mnb.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvs = cross_val_score(lg, data2, data1[:, 0], scoring=\"f1\", cv=5, n_jobs=-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96323086,  0.9599013 ,  0.96656535,  0.95555556,  0.95609153])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text + Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7044, 366), (7044, 4352812))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_matrix(data1[:, 1:]).shape, data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3 = hstack([coo_matrix(data1[:, 1:]), data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7044, 4353178)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data3, data1[:, 0], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=0.01)\n",
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89123071132856602"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lsvc.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(penalty=\"l1\", C=100)\n",
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = lg.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=400, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"data.txt\", 'w', \"utf-8\") as fout:\n",
    "    for id, item in data.items():\n",
    "        bag = []\n",
    "        target = -1 if item[\"target\"] == 0 else 1\n",
    "\n",
    "        for key in feature_tags:\n",
    "            if key in item[\"features\"].keys():\n",
    "                bag += [w for w in item[\"features\"][key][\"words\"] if len(w) < 10]\n",
    "\n",
    "\n",
    "        fout.write(u'{0} | {1}\\n'.format(target, ' '.join(bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 | яндекс круг поиск почта календарь деньги круг фотки народ открытки авто афиша блоги видео картинки карты каталог маркет музыка новости погода работа словари услуги ру программы сервисы войти помощь главная работа компании услуги вспомнить пароль создать профиль фото фото мобильная версия помощь обратная связь блог яндекс колл москва круг люди круге колл москва ещё м россия москва область москва е россия москва область москва\r\n",
      "-1 | яндекс круг поиск почта календарь деньги круг фотки народ открытки авто афиша блоги видео картинки карты каталог маркет музыка новости погода работа словари услуги ру программы сервисы войти помощь главная работа компании услуги вспомнить пароль создать профиль фото фото мобильная версия помощь обратная связь блог яндекс люди которые сайт круг люди круге ещё елена россия татарстан казань л россия петербург область санкт петербург например\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !vw --loss_function \"logistic\" -b 31 -d data.txt -c -k -f model.vw --passes 10 -l 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
