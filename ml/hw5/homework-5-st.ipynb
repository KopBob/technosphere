{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 5. Линейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import random as pr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cross_validation as cv\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачитываем результат 4 домашки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"../hw4/files/out_4.dat.npz\")\n",
    "users = data[\"users\"]\n",
    "X = data[\"data\"].reshape(1,)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачитываем категории пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_SET_URL = \"../hw1/twitter_train.csv\"\n",
    "df_users = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0, names=[\"twitter_id\", \"is_1\", \"is_2\", \"is_3\"], dtype={\"twitter_id\": str, \"is_1\": int, 'is_2': int, \"is_3\": int})\n",
    "df_users.set_index(\"twitter_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем целевую переменную: Делаем join списка пользователей из ДЗ4 с обучающей выборкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting training set: (3000x364911) feature matrix, 3000 target vector\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    if x[0] == 1:\n",
    "        return 1\n",
    "    if x[1] == 1:\n",
    "        return 2\n",
    "    if x[2] == 1:\n",
    "        return 3\n",
    "\n",
    "Y = df_users[['is_1', 'is_2', 'is_3']].apply(f, axis=1).values\n",
    "print \"Resulting training set: (%dx%d) feature matrix, %d target vector\" % (X.shape[0], X.shape[1], Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы исследовать, как ведут себя признаки, построим распределение количества ненулевых признаков у пользователей, чтобы убедиться, что он удовлетворяет закону Ципфа. Для этого построим гистограмму в логарифмических осях. [Подсказка](http://anokhin.github.io/img/sf1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAGvCAYAAAC97EO6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYOgIqwAAAAwdYRUAAIChI6wCAAAwdP5/tyy/8+2Og24AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cd13490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_log_hist(X):\n",
    "    \"\"\"\n",
    "    Draw tokens histogram in log scales\n",
    "    \"\"\"\n",
    "    y = np.array(X.sum(axis=0), dtype=np.int).ravel()\n",
    "    x = np.arange(y.size)\n",
    "    \n",
    "    plt.figure(figsize=(16, 7))\n",
    "    plt.xscale(\"log\", nonposx='clip')\n",
    "    plt.yscale(\"log\", nonposy='clip')\n",
    "    \n",
    "    plt.plot(x, np.sort(y)[::-1], \"rx\")\n",
    "    \n",
    "    return y\n",
    "\n",
    "features_counts = draw_log_hist(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем отбор признаков. В самом простом случае просто удаляем признаки, имеющие ненулевое значение у менее, чем 100 пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1 = X.tocsc()[:, features_counts > 100].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант задания генерируется на основании вашего ника в техносфере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My homework 5 algorithm is: Logistic regression with L1 regularization optimized by stochastic gradient descent\n"
     ]
    }
   ],
   "source": [
    "USER_NAME = \"b.kopin\"\n",
    "OPTIMIZATION_ALGORITHMS = [\"stochastic gradient descent\", \"Newton method\"]\n",
    "REGULARIZATIONS = [\"L1\", \"L2\"]\n",
    "\n",
    "print \"My homework 5 algorithm is: Logistic regression with %s regularization optimized by %s\" % (\n",
    "    REGULARIZATIONS[hash(USER_NAME) % 2],\n",
    "    OPTIMIZATION_ALGORITHMS[hash(USER_NAME[::-1]) % 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем выбранный алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_logistic(X, out=None):\n",
    "    \"\"\"Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n",
    "\n",
    "    This implementation is numerically stable because it splits positive and\n",
    "    negative values::\n",
    "\n",
    "        -log(1 + exp(-x_i))     if x_i > 0\n",
    "        x_i - log(1 + exp(x_i)) if x_i <= 0\n",
    "\n",
    "    For the ordinary logistic function, use ``sklearn.utils.fixes.expit``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array-like, shape (M, N) or (M, )\n",
    "        Argument to the logistic function\n",
    "\n",
    "    out: array-like, shape: (M, N) or (M, ), optional:\n",
    "        Preallocated output array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out: array, shape (M, N) or (M, )\n",
    "        Log of the logistic function evaluated at every point in x\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See the blog post describing this implementation:\n",
    "    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n",
    "    \"\"\"\n",
    "    is_1d = X.ndim == 1\n",
    "    X = np.atleast_2d(X)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if out is None:\n",
    "        out = np.empty_like(X)\n",
    "    \n",
    "    out = np.log(1 / (1 + np.e ** -X))\n",
    "\n",
    "    if is_1d:\n",
    "        return np.squeeze(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _logistic_loss_and_grad(w, X, y, alpha):\n",
    "    \"\"\"Computes the logistic loss and gradient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "        Logistic loss.\n",
    "\n",
    "    grad : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Logistic gradient.\n",
    "    \"\"\"\n",
    "    _, n_features = X.shape\n",
    "    grad = np.empty_like(w)\n",
    "\n",
    "    yz = y * X.dot(w)\n",
    "\n",
    "    # Logistic loss is the negative of the log of the logistic function.\n",
    "    out = -np.sum(log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
    "\n",
    "    z = 1.0 / (1.0 + np.exp(-yz))\n",
    "    z0 = (z - 1) * y\n",
    "\n",
    "    grad[:n_features] = X.T.dot(z0) + alpha * w\n",
    "\n",
    "    # Case where we fit the intercept.\n",
    "    if grad.shape[0] > n_features:\n",
    "        grad[-1] = z0.sum()\n",
    "    return out, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sigmoid(z):\n",
    "        \"\"\"\n",
    "        Return Sigmoid of z\n",
    "        Args:\n",
    "            z : ndarray or scalar\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "    scheme if the 'multi_class' option is set to 'ovr' and uses the\n",
    "    cross-entropy loss, if the 'multi_class' option is set to 'multinomial'.\n",
    "    (Currently the 'multinomial' option is supported only by the 'lbfgs' and\n",
    "    'newton-cg' solvers.)\n",
    "\n",
    "    This class implements regularized logistic regression using the\n",
    "    `liblinear` library, newton-cg and lbfgs solvers. It can handle both\n",
    "    dense and sparse input. Use C-ordered arrays or CSR matrices containing\n",
    "    64-bit floats for optimal performance; any other input format will be\n",
    "    converted (and copied).\n",
    "\n",
    "    The newton-cg and lbfgs solvers support only L2 regularization with primal\n",
    "    formulation. The liblinear solver supports both L1 and L2 regularization,\n",
    "    with a dual formulation only for the L2 penalty.\n",
    "\n",
    "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    penalty : str, 'l1' or 'l2'\n",
    "        Used to specify the norm used in the penalization. The newton-cg and\n",
    "        lbfgs solvers support only l2 penalties.\n",
    "\n",
    "    C : float, optional (default=1.0)\n",
    "        Inverse of regularization strength; must be a positive float.\n",
    "        Like in support vector machines, smaller values specify stronger\n",
    "        regularization.\n",
    "\n",
    "    fit_intercept : bool, default: True\n",
    "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "        added to the decision function.\n",
    "\n",
    "    class_weight : dict or 'balanced', optional\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           *class_weight='balanced'* instead of deprecated *class_weight='auto'*.\n",
    "\n",
    "    max_iter : int\n",
    "        Useful only for the newton-cg, sag and lbfgs solvers.\n",
    "        Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "    multi_class : str, {'ovr', 'multinomial'}\n",
    "        Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
    "        chosen is 'ovr', then a binary problem is fit for each label. Else\n",
    "        the loss minimised is the multinomial loss fit across\n",
    "        the entire probability distribution. Works only for the 'lbfgs'\n",
    "        solver.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : array, shape (n_classes, n_features)\n",
    "        Coefficient of the features in the decision function.\n",
    "\n",
    "    intercept_ : array, shape (n_classes,)\n",
    "        Intercept (a.k.a. bias) added to the decision function.\n",
    "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "\n",
    "    n_iter_ : array, shape (n_classes,) or (1, )\n",
    "        Actual number of iterations for all classes. If binary or multinomial,\n",
    "        it returns only 1 element. For liblinear solver, only the maximum\n",
    "        number of iteration across all classes is given.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, penalty='l1', tol=1e-4, C=1.0, B=0.5,\n",
    "                 fit_intercept=True, class_weight=None,\n",
    "                 max_iter=100, multi_class='ovr'):\n",
    "\n",
    "        self.penalty = penalty\n",
    "        self.tol = tol\n",
    "        self.C = C\n",
    "        self.B = B\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.class_weight = class_weight\n",
    "        self.max_iter = max_iter\n",
    "        self.multi_class = multi_class\n",
    "        \n",
    "    def _bin_fit(self, X, y=None):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        current_log_likelihood = None\n",
    "        self.converged_ = False\n",
    "       \n",
    "        coef_ = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            sys.stdout.write(\"\\r Iter = %i\" % (i))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            prev_log_likelihood = current_log_likelihood\n",
    "            \n",
    "            current_log_likelihood, grad = _logistic_loss_and_grad(coef_, X, y, 1.0 / self.C)\n",
    "\n",
    "            eta = (i + 1) ** (-self.B)\n",
    "            coef_ -= eta * grad\n",
    "            \n",
    "            # Check for convergence.\n",
    "            if np.all(np.abs(eta * grad) < self.tol):\n",
    "                self.converged_ = True\n",
    "                sys.stdout.flush()\n",
    "                break\n",
    "        \n",
    "        return coef_\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1), dtype=X.dtype), X))\n",
    "            \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.classes_ = np.unique(y)\n",
    "        m_classes = self.classes_.shape[0]\n",
    "        \n",
    "        self.coef_ = np.zeros((m_classes, n_features))\n",
    "        \n",
    "        for i, class_label in enumerate(self.classes_):\n",
    "            sys.stdout.write(\"\\r Class = %i \\n\" % (class_label))\n",
    "            \n",
    "            y_i = np.empty_like(y)[y == class_label] = 1\n",
    "            self.coef_[i, :] = self._bin_fit(X, y_i)\n",
    "        \n",
    "            sys.stdout.flush()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1), dtype=X.dtype), X))\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        m_classes = self.classes_.shape[0]\n",
    "        \n",
    "        multi_y = np.zeros((m_classes, n_samples))\n",
    "        \n",
    "        for i, class_label in enumerate(self.classes_):\n",
    "            multi_y[0, :] = (1.0 / (1.0 + np.exp(-X.dot(self.coef_[i]))))\n",
    "            \n",
    "        print multi_y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем метрику качества, используемую в соревновании: площадь под ROC кривой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class = 1 \n",
      " Class = 2 \n",
      " Class = 3 \n",
      " Iter = 57"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression instance at 0x11eba2cb0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = LogisticRegression(max_iter=1000)\n",
    "lg.fit(X1, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cv.train_test_split(X1, Y, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.83701910e+00,   1.44242074e-02,   6.46645923e-03, ...,\n",
       "          3.92202744e-05,   3.13064453e-03,   2.53970323e-05],\n",
       "       [  2.83701910e+00,   1.44242074e-02,   6.46645923e-03, ...,\n",
       "          3.92202744e-05,   3.13064453e-03,   2.53970323e-05],\n",
       "       [  2.83701910e+00,   1.44242074e-02,   6.46645923e-03, ...,\n",
       "          3.92202744e-05,   3.13064453e-03,   2.53970323e-05]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "lg.predict_proba(x_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auroc(y_prob, y_true):\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку с помощью методики кросс-валидации для того, чтобы настроить параметр регуляризации $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "C = [0.0, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "def select_reg_parameter(C, X, Y):\n",
    "    return C.index(max(C))\n",
    "\n",
    "index = select_reg_parameter(C, X1, Y)\n",
    "print index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем наилучшее значение $C$, и классифицируем неизвестных пользователей и строим ROC-кривую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X, Y, test_size, C):\n",
    "    Y_test = [1,2,3]*300\n",
    "    Y_prob = [[0.1,0.2,0.7]]*900\n",
    "    return Y_test, Y_prob\n",
    "\n",
    "def roc(Y_test, Y_prob, y_prob_ind, pos_label):\n",
    "    tpr = [1] * 2400\n",
    "    fpr = [0.01] * 2400\n",
    "    roc_auc = 0.51\n",
    "    \n",
    "    return tpr, fpr, roc_auc\n",
    "\n",
    "def plot_roc_curve(tpr, fpr, roc_auc):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    # Your code here\n",
    "    return\n",
    "\n",
    "Y_test, Y_prob = predict(X1, Y, 0.3, C[index1])\n",
    "\n",
    "tpr, fpr, roc_auc = roc(Y_test, Y_prob, 0, 1)\n",
    "print \"Area under the ROC curve : %f\" % roc_auc\n",
    "plot_roc_curve(tpr, fpr, roc_auc)\n",
    "\n",
    "tpr, fpr, roc_auc = roc(Y_test, Y_prob, 1, 2)\n",
    "print \"Area under the ROC curve : %f\" % roc_auc\n",
    "plot_roc_curve(tpr, fpr, roc_auc)\n",
    "\n",
    "tpr, fpr, roc_auc = roc(Y_test, Y_prob, 2, 3)\n",
    "print \"Area under the ROC curve : %f\" % roc_auc\n",
    "plot_roc_curve(tpr, fpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью полученной модели предсказываем категории для неизвестных пользователей из соревнования и загружаем на kaggle в нужном формате."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
