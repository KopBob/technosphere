{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pure_train_data = pd.read_csv(\"./data/train.data.cvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pure_train_data #[~pure_train_data.duplicated([\"QID\", \"Y\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_s = train_data.loc[:, [\"Y\"]]\n",
    "QID_s = train_data.loc[:, [\"QID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dt = train_data.drop([\"Y\", \"QID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info_dt = pd.concat([QID_s, Y_s], axis=1)\n",
    "info_dt = info_dt.sort_values([\"QID\", \"Y\"], ascending=[True, False])\n",
    "\n",
    "def numerate(group):\n",
    "    group.loc[:, \"num\"] = range(1, len(group) + 1)\n",
    "    return group\n",
    "\n",
    "info_dt = info_dt.groupby(\"QID\").apply(numerate)\n",
    "\n",
    "info_dt.loc[:, \"Y_norm\"] = info_dt.Y/info_dt.Y.max()\n",
    "\n",
    "info_dt.loc[:, \"DCG\"] = info_dt.apply(lambda x: (2**x.Y_norm - 1) / np.log2(x.num + 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_QID = pd.Series(info_dt.QID.unique())\n",
    "test_QIDs = unique_QID.sample(frac=0.5)\n",
    "train_QIDs = unique_QID[~unique_QID.isin(test_QIDs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_grouped_by_qid = info_dt.groupby(\"QID\")\n",
    "qid_dcg_s = info_dt.groupby([\"QID\", \"num\"]).DCG.first().reset_index().groupby(\"QID\").DCG.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_doc_ids = info_dt[info_dt.QID.isin(train_QIDs)].index\n",
    "test_doc_ids = info_dt[info_dt.QID.isin(test_QIDs)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = X_dt.loc[train_doc_ids]\n",
    "test_X = X_dt.loc[test_doc_ids]\n",
    "train_qid_dcg = qid_dcg_s.loc[train_QIDs]\n",
    "test_qid_dcg = qid_dcg_s.loc[test_QIDs]\n",
    "train_docs_info = info_dt.loc[train_doc_ids]\n",
    "test_docs_info = info_dt.loc[test_doc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_rel = lambda rel, pos: (2**rel - 1)/np.log2(pos + 1)\n",
    "f_sigm = lambda si, sj, sign: -0.5 / (1.0 + np.exp(0.5 *(si - sj)*sign)) * sign\n",
    "\n",
    "def compute_grad(curr_obj, obj, q_dcg):\n",
    "    bigger = obj[\"Y_norm\"] < curr_obj[\"Y_norm\"]\n",
    "    lower = obj[\"Y_norm\"] > curr_obj[\"Y_norm\"]\n",
    "    if not(lower or bigger):\n",
    "        return 0\n",
    "    \n",
    "    new_dcg = (q_dcg -\\\n",
    "                curr_obj[\"DCG\"] - obj[\"DCG\"] +\\\n",
    "                f_rel(curr_obj[\"Y_norm\"], obj[\"num\"]) + f_rel(obj[\"Y_norm\"], curr_obj[\"num\"]))\n",
    "    \n",
    "    ndcg_delta = np.abs(1 - new_dcg/q_dcg)\n",
    "    \n",
    "    sign = 1 if bigger else -1\n",
    "    grad = f_sigm(0, 0, sign)\n",
    "    lmbda = grad * ndcg_delta\n",
    "    return -lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_qid_idcg = test_docs_info.groupby(\"QID\")[\"DCG\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dcg(group):\n",
    "    group = group.reset_index()\n",
    "    dcg = ((2**group.Y_norm - 1)/np.log2(group.index + 2)).sum()\n",
    "    return dcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_lambda(rel, pred_rel, sigma = 0.8):\n",
    "    rel_sign = np.sign(rel[:, np.newaxis] - rel)\n",
    "    pred_rel_diff = pred_rel[:, np.newaxis] - pred_rel\n",
    "    pred_rel_sign = np.sign(pred_rel_diff)\n",
    "    \n",
    "    left_part = 0.5*(1.0 - rel_sign) # === 0 т.к. мы рассматриваем только Sij = 1\n",
    "    right_part = 1.0/(1.0 + np.exp(sigma * pred_rel_diff * rel_sign))\n",
    "    \n",
    "    lambda_m = sigma*(-right_part)*rel_sign\n",
    "    \n",
    "    return lambda_m.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_docs_grads(qid_group):\n",
    "    qid_doc_ids = qid_group.index.tolist()\n",
    "    qid_rels = qid_group[\"Y_norm\"].as_matrix()\n",
    "    qid_rels_pred = qid_group[\"pred\"].as_matrix()\n",
    "\n",
    "    qid_grad = -compute_lambda(qid_rels, qid_rels_pred)\n",
    "\n",
    "    return zip(qid_doc_ids, qid_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train estimator #1\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.728280982794\n",
      "Train estimator #2\n",
      " 3000/3000\n",
      "Train estimator #3\n",
      " 3000/3000\n",
      "Train estimator #4\n",
      " 3000/3000\n",
      "Train estimator #5\n",
      " 3000/3000\n",
      "Train estimator #6\n",
      " 3000/3000\n",
      "Train estimator #7\n",
      " 3000/3000\n",
      "Train estimator #8\n",
      " 3000/3000\n",
      "Train estimator #9\n",
      " 3000/3000\n",
      "Train estimator #10\n",
      " 3000/3000\n",
      "Train estimator #11\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.758359507128\n",
      "Train estimator #12\n",
      " 3000/3000\n",
      "Train estimator #13\n",
      " 3000/3000\n",
      "Train estimator #14\n",
      " 3000/3000\n",
      "Train estimator #15\n",
      " 3000/3000\n",
      "Train estimator #16\n",
      " 3000/3000\n",
      "Train estimator #17\n",
      " 3000/3000\n",
      "Train estimator #18\n",
      " 3000/3000\n",
      "Train estimator #19\n",
      " 3000/3000\n",
      "Train estimator #20\n",
      " 3000/3000\n",
      "Train estimator #21\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.763526033595\n",
      "Train estimator #22\n",
      " 3000/3000\n",
      "Train estimator #23\n",
      " 3000/3000\n",
      "Train estimator #24\n",
      " 3000/3000\n",
      "Train estimator #25\n",
      " 3000/3000\n",
      "Train estimator #26\n",
      " 3000/3000\n",
      "Train estimator #27\n",
      " 3000/3000\n",
      "Train estimator #28\n",
      " 3000/3000\n",
      "Train estimator #29\n",
      " 3000/3000\n",
      "Train estimator #30\n",
      " 3000/3000\n",
      "Train estimator #31\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.765982745056\n",
      "Train estimator #32\n",
      " 3000/3000\n",
      "Train estimator #33\n",
      " 3000/3000\n",
      "Train estimator #34\n",
      " 3000/3000\n",
      "Train estimator #35\n",
      " 3000/3000\n",
      "Train estimator #36\n",
      " 3000/3000\n",
      "Train estimator #37\n",
      " 3000/3000\n",
      "Train estimator #38\n",
      " 3000/3000\n",
      "Train estimator #39\n",
      " 3000/3000\n",
      "Train estimator #40\n",
      " 3000/3000\n",
      "Train estimator #41\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.767007853637\n",
      "Train estimator #42\n",
      " 3000/3000\n",
      "Train estimator #43\n",
      " 3000/3000\n",
      "Train estimator #44\n",
      " 3000/3000\n",
      "Train estimator #45\n",
      " 3000/3000\n",
      "Train estimator #46\n",
      " 3000/3000\n",
      "Train estimator #47\n",
      " 3000/3000\n",
      "Train estimator #48\n",
      " 3000/3000\n",
      "Train estimator #49\n",
      " 3000/3000\n",
      "Train estimator #50\n",
      " 3000/3000\n",
      "Train estimator #51\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.767910150976\n",
      "Train estimator #52\n",
      " 3000/3000\n",
      "Train estimator #53\n",
      " 3000/3000\n",
      "Train estimator #54\n",
      " 3000/3000\n",
      "Train estimator #55\n",
      " 3000/3000\n",
      "Train estimator #56\n",
      " 3000/3000\n",
      "Train estimator #57\n",
      " 3000/3000\n",
      "Train estimator #58\n",
      " 3000/3000\n",
      "Train estimator #59\n",
      " 3000/3000\n",
      "Train estimator #60\n",
      " 3000/3000\n",
      "Train estimator #61\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.767802736415\n",
      "Train estimator #62\n",
      " 3000/3000\n",
      "Train estimator #63\n",
      " 3000/3000\n",
      "Train estimator #64\n",
      " 3000/3000\n",
      "Train estimator #65\n",
      " 3000/3000\n",
      "Train estimator #66\n",
      " 3000/3000\n",
      "Train estimator #67\n",
      " 3000/3000\n",
      "Train estimator #68\n",
      " 3000/3000\n",
      "Train estimator #69\n",
      " 3000/3000\n",
      "Train estimator #70\n",
      " 3000/3000\n",
      "Train estimator #71\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.767697549209\n",
      "Train estimator #72\n",
      " 3000/3000\n",
      "Train estimator #73\n",
      " 3000/3000\n",
      "Train estimator #74\n",
      " 3000/3000\n",
      "Train estimator #75\n",
      " 3000/3000\n",
      "Train estimator #76\n",
      " 3000/3000\n",
      "Train estimator #77\n",
      " 3000/3000\n",
      "Train estimator #78\n",
      " 3000/3000\n",
      "Train estimator #79\n",
      " 3000/3000\n",
      "Train estimator #80\n",
      " 3000/3000\n",
      "Train estimator #81\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.768227481821\n",
      "Train estimator #82\n",
      " 3000/3000\n",
      "Train estimator #83\n",
      " 3000/3000\n",
      "Train estimator #84\n",
      " 3000/3000\n",
      "Train estimator #85\n",
      " 3000/3000\n",
      "Train estimator #86\n",
      " 3000/3000\n",
      "Train estimator #87\n",
      " 3000/3000\n",
      "Train estimator #88\n",
      " 3000/3000\n",
      "Train estimator #89\n",
      " 3000/3000\n",
      "Train estimator #90\n",
      " 3000/3000\n",
      "Train estimator #91\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.768495759321\n",
      "Train estimator #92\n",
      " 3000/3000\n",
      "Train estimator #93\n",
      " 3000/3000\n",
      "Train estimator #94\n",
      " 3000/3000\n",
      "Train estimator #95\n",
      " 3000/3000\n",
      "Train estimator #96\n",
      " 3000/3000\n",
      "Train estimator #97\n",
      " 3000/3000\n",
      "Train estimator #98\n",
      " 3000/3000\n",
      "Train estimator #99\n",
      " 3000/3000\n",
      "Train estimator #100\n",
      " 3000/3000\n",
      "Train estimator #101\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.769585017681\n",
      "Train estimator #102\n",
      " 3000/3000\n",
      "Train estimator #103\n",
      " 3000/3000\n",
      "Train estimator #104\n",
      " 3000/3000\n",
      "Train estimator #105\n",
      " 3000/3000\n",
      "Train estimator #106\n",
      " 3000/3000\n",
      "Train estimator #107\n",
      " 3000/3000\n",
      "Train estimator #108\n",
      " 3000/3000\n",
      "Train estimator #109\n",
      " 3000/3000\n",
      "Train estimator #110\n",
      " 3000/3000\n",
      "Train estimator #111\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.76929944791\n",
      "Train estimator #112\n",
      " 3000/3000\n",
      "Train estimator #113\n",
      " 3000/3000\n",
      "Train estimator #114\n",
      " 3000/3000\n",
      "Train estimator #115\n",
      " 3000/3000\n",
      "Train estimator #116\n",
      " 3000/3000\n",
      "Train estimator #117\n",
      " 3000/3000\n",
      "Train estimator #118\n",
      " 3000/3000\n",
      "Train estimator #119\n",
      " 3000/3000\n",
      "Train estimator #120\n",
      " 3000/3000\n",
      "Train estimator #121\n",
      " 3000/3000\n",
      "  validate\n",
      "ndcg: 0.769158936452\n",
      "Train estimator #122\n",
      " 3000/3000\n",
      "Train estimator #123\n",
      " 3000/3000\n",
      "Train estimator #124\n",
      " 3000/3000\n",
      "Train estimator #125\n",
      " 3000/3000\n",
      "Train estimator #126\n",
      " 3000/3000\n",
      "Train estimator #127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c99a92a6569f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mlrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambdaRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrikage_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mlrk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_doc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_QIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_qid_dcg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_docs_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_docs_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_qid_idcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c99a92a6569f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, _train_X, _train_doc_ids, _train_QIDs, _train_qid_dcg, _train_docs_info, test_X, test_docs_info, test_qid_idcg)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train estimator #%i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mensemble_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_train_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0m_train_docs_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_train_doc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c99a92a6569f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrikage_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bkopin/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bkopin/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[1;32m    367\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[0;32m/Users/bkopin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    371\u001b[0m                                       force_all_finite)\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LambdaRank:\n",
    "    def __init__(self, n_estimators, shrikage_rate=0.01):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.trees = []\n",
    "        self.shrikage_rate = shrikage_rate\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            y += tree.predict(X) * self.shrikage_rate\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    def fit(self, _train_X, _train_doc_ids, _train_QIDs, _train_qid_dcg, _train_docs_info,\n",
    "           test_X, test_docs_info, test_qid_idcg):\n",
    "        base_tree = DecisionTreeRegressor(max_depth=4)\n",
    "        base_tree.fit(_train_X.as_matrix(), np.tile(0, (len(_train_X),1)))\n",
    "        \n",
    "        self.trees.append(base_tree)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            print(\"Train estimator #%i\" % (i + 1))\n",
    "            \n",
    "            ensemble_predict = self.predict(_train_X.as_matrix())\n",
    "            _train_docs_info.loc[_train_doc_ids, \"pred\"] = ensemble_predict\n",
    "            \n",
    "            docs_grouped_by_qid = _train_docs_info.groupby(\"QID\")\n",
    "            \n",
    "            docs_grads = []\n",
    "            \n",
    "#             pool = multiprocessing.Pool(processes=8)\n",
    "#             docs_grads = []\n",
    "#             data_gen = (docs_grouped_by_qid.get_group(q_id) for q_id in _train_QIDs)\n",
    "#             for j, part in enumerate(pool.imap_unordered(compute_docs_grads, data_gen)):\n",
    "#                 docs_grads += part\n",
    "#             pool.close()\n",
    "            \n",
    "            for j, q_id in enumerate(_train_QIDs):\n",
    "                sys.stdout.write(\"\\r %s/%s\" %(j+1, len(train_QIDs)))\n",
    "                \n",
    "                qid_group = docs_grouped_by_qid.get_group(q_id)\n",
    "                \n",
    "                qid_doc_ids = qid_group.index.tolist()\n",
    "                qid_rels = qid_group[\"Y_norm\"].as_matrix()\n",
    "                qid_rels_pred = qid_group[\"pred\"].as_matrix()\n",
    "                \n",
    "                qid_grad = -compute_lambda(qid_rels, qid_rels_pred)\n",
    "                \n",
    "                docs_grads += zip(qid_doc_ids, qid_grad)\n",
    "            print()\n",
    "            tree = DecisionTreeRegressor(max_depth=4, min_samples_split=4)\n",
    "            step_train_doc_ids, step_train_grad = zip(*docs_grads)\n",
    "            tree = tree.fit(_train_X.loc[list(step_train_doc_ids)].as_matrix(), np.asarray(step_train_grad))\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(\"  validate\")\n",
    "                test_docs_info.loc[test_doc_ids, \"test_pred_y\"] = lrk.predict(test_X.as_matrix())\n",
    "\n",
    "                test_qid_dcg = test_docs_info\\\n",
    "                    .sample(frac=1)\\\n",
    "                    .sort_values([\"QID\", \"test_pred_y\"], ascending=[True, False])\\\n",
    "                    .groupby(\"QID\")\\\n",
    "                    .apply(compute_dcg)\n",
    "                print(\"ndcg:\", (test_qid_dcg/test_qid_idcg).mean())\n",
    "            \n",
    "lrk = LambdaRank(n_estimators=250, shrikage_rate=0.02)\n",
    "lrk.fit(train_X, train_doc_ids, train_QIDs, train_qid_dcg, train_docs_info, test_X, test_docs_info, test_qid_idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open(\"./simple_lambda_rank.model\", 'wr') as fout:\n",
    "    dill.dump(lrk, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validate\n",
      "ndcg: 0.769064834942\n"
     ]
    }
   ],
   "source": [
    "print(\"  validate\")\n",
    "test_docs_info.loc[test_doc_ids, \"test_pred_y\"] = lrk.predict(test_X.as_matrix())\n",
    "# test_docs_info.loc[test_doc_ids, \"test_pred_y\"] = np.random.normal(loc=0, scale=1.0, size=len(test_X))\n",
    "\n",
    "test_qid_dcg = test_docs_info.sample(frac=1)\\\n",
    "    .sort_values([\"QID\", \"test_pred_y\"], ascending=[True, False])\\\n",
    "    .groupby(\"QID\")\\\n",
    "    .apply(compute_dcg)\n",
    "print(\"ndcg:\", (test_qid_dcg/test_qid_idcg).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = pd.read_csv(\"./data/testset.cvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test_QID = testset[\"QID\"]\n",
    "Test_X = testset.drop([\"Y\", \"QID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test_rel = lrk.predict(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumbission = Test_QID.to_frame().copy()\n",
    "sumbission.loc[:, \"rel\"] = Test_rel\n",
    "# sumbission.loc[:, \"rel\"] = np.random.normal(loc=0, scale=1.0, size=len(Test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumbission.index.name = \"DocumentId\"# ,QueryId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumbission = sumbission.reset_index()\\\n",
    "        .sample(frac=1)\\\n",
    "        .sort_values([\"QID\", \"rel\"], ascending=[True, False])\n",
    "sumbission = sumbission.rename(columns={\n",
    "        \"QID\": \"QueryId\"\n",
    "    })\n",
    "sumbission.loc[:, \"DocumentId\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumbission[[\"DocumentId\", \"QueryId\"]].to_csv(\"./data/sumbission7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumbission.groupby(\"QueryId\").rel.count().min()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
